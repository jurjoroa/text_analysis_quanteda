---
title: "IDS Workshop - Quanteda"
subtitle: "How we met Quanteda - Analyzing the TV show ‘How I Met Your Mother’ with quanteda"
author: "Alexander Kraess, Augusto Fonseca, Jorge Roa"
date: "21/11/2022"
output: 
  html_document:
    toc: TRUE
    df_print: paged
    number_sections: FALSE
    highlight: tango
    theme: lumen
    toc_depth: 3
    toc_float: true
    self_contained: false
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
rm(list = ls()) # to clean the workspace
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

```

------------------------------------------------------------------------
```{r, fig.align='left', echo=F, out.width = "30%"}
knitr::include_graphics("images/hertie_logo.png")
```



```{r, fig.align='center', echo=F, out.width = "60%"}
knitr::include_graphics("images/himym.png")
```

# 1. Welcome the exercises for Quanteda!

Quanteda is a brilliant package, and we hope that we can show you this power fullness! 
In this session You will practice the basic concepts, some filters and also some visualization features.

Fell free to access our GitHub page (https://github.com/jurjoroa/text_analysis_quanteda) to check our code and other details.


**Step 1** Load the packages which we will use (don't forget to install them before!).
```{r, message=F}
library(tidyverse)
library(tidytext)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(stringr)
library(spacyr)
library(ggsci)
library(ggrepel)
library(RColorBrewer)
library(cowplot)
library(magick)
library(gghighlight)
library(readtext)
library(rvest)
library(xml2)
library(polite)
library(httr) #Package for working with HTTP organised by HTTP verbs 
```

# 2. We will skip the scraping part and will directly jump to our data. You can load the data from our github-repository


**Step 2.1** Load the files "df_himym_final_doc.Rdata" and "df_characters_w.Rdata" from data folder.

```{r, message=F}
## 03.1- Load data- -----------------------------------------------------------
#If you want to know how we generated this data, go to the session 2 (or even the script 02_web_scrap)
obj_img <- image_read(path = "https://bit.ly/3twmH2Y")
load("data/df_himym_final_doc.Rdata")
load("data/df_characters_w.Rdata")


```

# 3. Uploading data into Quanteda corpus

OK! It's showtime! Let's upload all our data into a Quanteda corpus element.

**Step 3.1** First Step: Define a corpus "corp_himym", change the doc names to the name of the episode and them show the corpus summary.

```{r, message=F}
# 01.- Quanteda analysis -------------------------------------------------------
# 02.- First step: Define a corpus ---------------------------------------------
corp_himym <- corpus(df_himym_final_doc)  #Build a new corpus from the texts
docnames(corp_himym) <- df_himym_final_doc$Title
summary(corp_himym, n = 15)
```
**Step 3.2** Now, let's "tokenize" it!
Convert corpus into tokens and wrangle it.

A- Choose one season and filter the corpus element created to show only the documents for this season. Check it with the summary function.
B - tokenize the corpus subsetted and call it "toks_himym_s1". Show it's content.
C- Use the parameter "remove_punct" to remove the punctuation elements from this token.  Show it's content. 
D- Use the parameter "remove_separators" to remove the separator elements from this token.  Show it's content.
E- Use the parameter "remove_numbers" to remove the number elements from this token.  Show it's content.
F- Use the parameter "remove_symbols" to remove the symbol elements from this token.  Show it's content.
G- Use the function "tokens_remove" to remove the stopword elements from this token.  Show it's content.


Did you realize the difference? Now we can focus just on the words!

```{r}
# 03.- Second step: Convert corpus into tokens and wrangle it ------------------
corp_himym_stat <- corp_himym
corp_himym_s1_simil <- corpus_subset(corp_himym_stat, Season == 1) #We want to analyze just the first season
toks_himym_s1 <- tokens(corp_himym_s1_simil, #corpus from all the episodes from the first season
                        remove_punct = TRUE, #Remove punctuation of our texts
                        remove_separators = TRUE, #Remove separators of our texts
                        remove_numbers = TRUE, #Remove numbers of our texts
                        remove_symbols = TRUE) %>% #Remove symbols of our texts
  tokens_remove(stopwords("english")) #Remove stop words of our texts


```

**Step 3.3** Create a DFM from the token "toks_himym_s1" and name it "toks_himym_dm_s1"

```{r}
# 03.- Third step: Convert our tokens into a Document Feature Matrix -----------
toks_himym_dm_s1 <- toks_himym_s1 %>% 
                    dfm()
```


Let's check the new subsetted DFM. Is it like the corpus element?


```{r}
toks_himym_dm_s1
```

# 4. Now... let's have fun!

Now that we already upload the data and created the Quanteda elements, we can try some basics analysis.



**Step 4.1** Have you ever thought that you had seen an episode before? 
Let's check the similarity between episodes! Use the function "textstat_simil" to check the similarity between the episodes and plot it.



```{r}
# 05.- Similarity between episodes --------------------------------------------
## 05.01.- textstat_simil function- --------------------------------------------
tstat_simil <- textstat_simil(toks_himym_dm_s1) #Check similarity between episodes of the first season
clust <- hclust(as.dist(tstat_simil)) #Convert our object into a cluster (For visualization purposes)
dclust <- as.dendrogram(clust)  #Convert our cluster into a dendogram (For visualization purposes)
dclust <- reorder(dclust, 1:22) #Order our visualization
#Seetle colors
nodePar <- list(lab.cex = 1, pch = c(NA, 19), 
                cex.axis = 1.5,
                cex = 2, col = "#0080ff")
                
## 04.02.- Plot Similarity between episodes--------------------------------------------
#Talk about different methods above the correlation 
par(mar = c(15, 7, 2, 1))
#Plot dendogram
plot(dclust, nodePar = nodePar,
     las = 1,
     cex.lab = 2, cex.axis = 2, cex.main = 2, cex.sub = 2,
     main = "How I Met Your Mother Season 1",
     type = "triangle",
     ylim = c(0,1),
     ylab = "Similarity between episodes (correlation %)",
     edgePar = list(col = 4:7, lwd = 7:7),
     panel.first = abline(h = c(seq(.10, 1, .10)), col = "grey80"))
rect.hclust(clust, k = 5, border = "red")
```

**Step 4.3**  Is there a correlation between the episodes? Use the function "textstat_dist" to check the distance between the episodes.



```{r}
# 06.- Distance between episodes (by correlation) ------------------------------
## 06.01.- textstat_dist function- ---------------------------------------------
tstat_dist <- textstat_dist(toks_himym_dm_s1)
clust_dist <- hclust(as.dist(tstat_dist))
dclust_dist <- as.dendrogram(clust_dist)
dclust_dist <- reorder(dclust_dist, 22:1)
nodePar_2 <- list(lab.cex = 1.2, pch = c(NA, 19), 
                  cex = 1.8, col = 11)
## 06.02.- Plot Distance between episodes (by correlation)----------------------
par(mar = c(15,7,2,1))
plot(dclust_dist, nodePar = nodePar_2,
     cex.lab = 2, cex.axis = 2, cex.main = 2, cex.sub = 2,
     main = "How I Met Your Mother Season 1",
     type = "triangle", ylim = c(0, 120),
     ylab = "Distance between episodes (correlation %)",
     edgePar = list(col = 11:19, lwd = 7:7),
     panel.first = abline(h = c(seq(10, 120, 10)), col = "grey80"))
rect.hclust(clust_dist, k = 5, border = "red")
```


**Step 4.4** What is the main actor for you? Does it depend on the season?

A- Use the corpus that contain all seasons (corp_himym) and tokenize it, removing punctuation, separators, numbers, symbols and stopwords and save it as "toks_himym".

B- You can use the function "tokens_select" to select just the main HIMYM characters ("Ted", "Marshall", "Lily", "Robin", "Barney", "Mother"). After that, group it by season using the function "tokens_group" and create a DFM naming it as "dfm_actors". 

C - Use the function "textstat_frequency" to check the frequency of each character.

D - Plot it with ggplot.


```{r}
# 07.- Appearances of actors by season------------------------------------------

## 07.01.- Characters by season--------------------------------------------------
#Remember our second step: tokenize our corpus. 
toks_himym <- tokens(corp_himym, #corpus from all the episodes from all season
                     remove_punct = TRUE, #Remove punctuation of our texts
                     remove_separators = TRUE,  #Remove separators of our texts
                     remove_numbers = TRUE, #Remove numbers of our texts
                     remove_symbols = TRUE) %>% #Remove symbols of our texts
  tokens_remove(stopwords("english")) #remove further stopwords
#Remember our third step: DFM object
dfm_actors <- toks_himym %>% 
  tokens_select(c("Ted", "Marshall", "Lily", "Robin", "Barney", "Mother")) %>% #We just want to analyze these characters
  tokens_group(groups = Season) %>% #We group our tokens (scripts) by season
  dfm() #Transform the token into a DFM object
  
  
## 07.02.- textstat_frequency function------------------------------------------
df_final_actors <-  as.data.frame(textstat_frequency(dfm_actors, groups = c(1:9))) %>% 
                    mutate(Season = paste("Season", group),
                           `Principal Characters` = replace(feature, is.character(feature), str_to_title(feature))) %>% 
                    select(-feature)
                    
                    
## 07.03.- Plot frequency of actors--------------------------------------------
#We now use ggplot to visualize the frequency of the actors 
ggplot1 <- ggplot(df_final_actors, aes(x = group, y = frequency, group = `Principal Characters`, color = `Principal Characters`)) +
  geom_line(size = 1.5) +
  scale_color_manual(values = brewer.pal(n = 6, name = "Dark2")) +
  geom_point(size = 3.2) +
  scale_y_continuous(breaks = seq(0, 5600, by = 50), limits = c(0,560))+
  theme_minimal(base_size = 14) +
  labs(x = "Number of Season",
       y = "Frequencies of appreances",
       title = "Appearances of principal characters by Season",
       caption="Description: This plot show the number of times that the \n principal characters appears in HIMYM per season.")+
       theme(panel.grid.major=element_line(colour="#cfe7f3"),
             panel.grid.minor=element_line(colour="#cfe7f3"),
             plot.title = element_text(margin = margin(t = 10, r = 20, b = 30, l = 30)),
             #axis.text.x=element_text(size=15),
             #axis.text.y=element_text(size=15),
             plot.caption=element_text(size=12, hjust=.1, color="#939393"),
             legend.position="bottom",
             plot.margin = margin(t = 20,  # Top margin
                                  r = 50,  # Right margin
                                  b = 40,  # Bottom margin
                                  l = 10), # Left margin
             text=element_text(family="sans")) + 
#geom_segment(aes(x = 8.5, y = 75, xend = 8.8, yend = 70),
#             arrow = arrow(length = unit(0.1, "cm")))+
  guides(colour = guide_legend(ncol = 6))
ggdraw(ggplot1) + draw_image(obj_img, x = .97, y = .97, 
                               hjust = 1.1, vjust = .7, 
                               width = 0.11, height = 0.1)
RColorBrewer::brewer.pal(n = 7, name = "Set1")
```

**Step 5.5** What is the most frequent character that appears in the TV show?
Use the function "textplot_wordcloud" to plot it as a wordcloud chart.


```{r}
# 08.- Wordcloud of PRINCIPAL characters that appears in HIMYM------------------
## 08.01.- Wordcloud steps------------------------------------------------------
### 08.01.01.- Second step: Tokens----------------------------------------------
toks_himym_characters <- tokens(corp_himym, #corpus from all the episodes from all season
                                remove_punct = TRUE, #Remove punctuation of our texts
                                remove_separators = TRUE, #Remove separators of our texts
                                remove_numbers = TRUE, #Remove numbers of our texts
                                remove_symbols = TRUE) %>% #Remove symbols of our texts
  tokens_keep(c(unique(df_characters_w$name))) #This function allow us to keep just the tokens that we want. 
#In this case, we just want the characters.
### 08.01.02.- Third step: DFM object-------------------------------------------
dfm_general_characters <- toks_himym_characters %>%
                          dfm()
## 08.02.- Generate Wordcloud --------------------------------------------------
textplot_wordcloud(dfm_general_characters, 
                   rotation = 0.25,
                   font = "sans",
                   min_count = 1, #Minimum frequency
                   color = brewer.pal(11, "RdBu"))
#RColorBrewer::display.brewer.all()
```

**Step 4.6** What about the SECONDARY characters?
Try to make  the same approach, not considering  the main characters("Ted", "Barney", "Lily", "Robin", "Marshall")


```{r}
# 09.- Wordcloud of SECONDARY characters that appears in HIMYM------------------
## 09.01.- Wordcloud steps------------------------------------------------------
### 09.01.01.- Second step: Tokens----------------------------------------------
toks_himym_sec_characters <- tokens(corp_himym, #corpus from all the episodes from all season
                                    remove_punct = TRUE, #Remove punctuation of our texts
                                    remove_separators = TRUE, #Remove separators of our texts
                                    remove_numbers = TRUE, #Remove numbers of our texts
                                    remove_symbols = TRUE) %>% #Remove symbols of our texts
  tokens_keep(c(unique(df_characters_w$name))) %>% #We want to keep all the characters
  tokens_remove(c("Ted", "Barney", "Lily", "Robin", "Marshall")) #But we remove the principal characters
### 09.01.02.- Third step: DFM object-------------------------------------------
dfm_general_sec_characters <- toks_himym_sec_characters %>%
                              dfm()
## 09.02.- Generate Wordcloud --------------------------------------------------
textplot_wordcloud(dfm_general_sec_characters, 
                   random_order = FALSE, 
                   rotation = 0.25,
                   #comparison = TRUE,
                   labelsize = 1.5,
                   min_count = 1, #Minimum frequency
                   color = RColorBrewer::brewer.pal(8, "Spectral"))
```

# 6. Network plots

**Step 6.1** Now let's just show you some powerful features for visualizing relationships using a network graph



```{r}
# How are the characters related each other? 
# We can answer this question with network plots and use quanteda for it

## 11.01.- Network steps------------------------------------------------------
### 11.01.01.- Second step: Tokens----------------------------------------------
token_characters_himym <- tokens(corp_himym, #corpus from all the episodes from all season
                                 remove_punct = TRUE, #Remove punctuation of our texts
                                 remove_separators = TRUE, #Remove separators of our texts
                                 remove_numbers = TRUE, #Remove numbers of our texts
                                 remove_symbols = TRUE) %>%  #Remove symbols of our texts
  tokens_keep(c(unique(df_characters_w$name))) %>% #We want to keep all the characters
  tokens_tolower() #We want lower cases in our tokens
### 11.01.02.- Extra step: create a feature co-ocurrence matrix (FCM)------------
fcm_characters_himym <- token_characters_himym %>%
                        fcm(context = "window", window = 5, tri = FALSE) # Window is a variable for the size of a window on either side of the target feature
                       #if tri = TRUE it would only return the upper triangle
                       
## 11.02.- Network plot of all characters----------------------------------------
#Vector with all the characters
v_top_characters <- stringr::str_to_sentence(names(topfeatures(fcm_characters_himym, 70)))
set.seed(100)
textplot_network(fcm_select(fcm_characters_himym, v_top_characters),
                 edge_color = "#008eed", 
                 edge_size = 2, 
                 vertex_labelcolor = "#006fba", 
                 omit_isolated = TRUE,
                 min_freq = .1)
```


**Step 6.2** Network plot with 30 principal characters



```{r}
## 11.03.- Network plot with 30 principal characters----------------------------
#Vector with 30 characters
v_top_characters_2 <- stringr::str_to_sentence(names(topfeatures(fcm_characters_himym, 30)))
textplot_network(fcm_select(fcm_characters_himym, v_top_characters_2),
                 edge_color = "#008eed", 
                 edge_size = 5, 
                 vertex_labelcolor = "#006fba",
                 omit_isolated = TRUE,
                 min_freq = .1)
```


**Step 6.3** Network plot of Ted



```{r}
## 11.03.- Network plot of Ted -------------------------------------------------
fcm_characters_himym_ted <- token_characters_himym %>%
  tokens_remove(c("marshall", "lily", "barney", "robin")) %>% #Here we just want ted, that why we remove the other principal characters
  fcm(context = "window", window = 5, tri = FALSE)
#Vector with 30 characters
v_top_characters_3 <- stringr::str_to_sentence(names(topfeatures(fcm_characters_himym_ted, 30)))
#Create a FCM matrix with our characters
vertex_size_f <- fcm_select(fcm_characters_himym_ted, pattern = v_top_characters_3)
#Create a proportion 
v_proportion <- rowSums(vertex_size_f)/min(rowSums(vertex_size_f))
#Vector of Ted
x_p <- c("ted")
#Replace that proportion in our vector
final_v <- replace(v_proportion, names(v_proportion) %in% x_p, 
                   v_proportion[names(v_proportion) %in% x_p]/15)
textplot_network(fcm_select(fcm_characters_himym_ted, v_top_characters_3),
                 edge_color = "#008eed", 
                 edge_size = 5, 
                 vertex_labelcolor = "#006fba",
                 omit_isolated = TRUE,
                 vertex_labelsize = final_v,
                 min_freq = .1)
```

```


